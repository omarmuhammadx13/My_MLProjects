{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b1b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "لينك الفيديو اللي استخدمته : https://youtu.be/Z_mEL5DIOgg?si=3mZ2-CqPp3eF_RBz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import deepspeech\n",
    "import numpy as np\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd95ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def extract_audio(video_path, audio_output_path):\n",
    "    audio_file = audio_output_path + '\\\\output_audiox.mp3'\n",
    "    command = ['ffmpeg', '-i', video_path, '-ac', '1', '-ar', '16000', '-y', audio_file]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "extract_audio(\n",
    "    r'C:\\Users\\Omar Muhammed\\Downloads\\a.mp4',\n",
    "    r'C:\\Users\\Omar Muhammed'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35fd8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def convert_mp3_to_wav(mp3_file_path, wav_file_path):\n",
    "    # Load the MP3 file\n",
    "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
    "    # Export as WAV\n",
    "    audio.export(wav_file_path, format=\"wav\")\n",
    "\n",
    "mp3_file_path = r'C:\\Users\\Omar Muhammed\\output_audiox.mp3'\n",
    "wav_file_path = r'C:\\Users\\Omar Muhammed\\output_audiox.wav'\n",
    "convert_mp3_to_wav(mp3_file_path, wav_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738be0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Web Speech thinks you said (in Arabic): جدعنه المصريين عايز اذكر ايضا الاخ احمد عباس الصديق ايضا عايز اشكر جدا الاعلامي الاستاذ معتز بالله عبد الفتاح الثلاثه دول عملوا مجهود كبير جدا وفضلوا واقفين جنبه اكثر من اخواته اكثر من عيلته وكان باسم يوسف واحمد عباس الاثنين اللي كان مغلق بنسبه 100% انا بشكر كل الناس اللي اهتموا باخويا لانه اخويا ده عماد زي ابويا بالضبط فطبعا الحمد لله اخويا عادل كان موجود في ابوظبي راح زوجته جنبه محمد ابنه سافر له الحمد لله ويحط ايضا بالعائله كلها هناك الحمد لله هو افضل الان بشكر كل الناس اللي اتصلت تسال وتطمن عماد من الشخصيات المحبوبه المحترمه اللي لها الكثير من المعجبين والذين يحترموه مره اخرى بشكر دبي بشكر دوله الامارات على هذا الاهتمام وانت بتشوف مظاهر رقيه دوله يعني كل اللي انا طبعا هو الموضوع انساني جدا لكن انظر له نظره اخرى مظاهر رقيه دوله هذا الرجل كان ممكن ان يعني ان يكون مصيره مصير اخر لا قدر الله لولا هذا الاهتمام هذه السياره المستعده هذا المستشفى العظيم كل هذه الاشياء بشكرهم عليها وان شاء الله بالصحه والعافيه اخويا عماد بس كان لازم اشكر كل الناس اللي هي اهتمت جدا يعني رغبه غير الجدعنه والشهامه حضرتك يمكن انا وباسم ليست جيده جدا ولكن له كل الاحترام على اللي عمله يعني انا اظن انه هذا الموقف بالنسبه لي انا اشيله لباسم هشيله له يعني اشيله له في رقبتي يوم ما باسم يحتاج حاجه لي دين في رقبتي والاستاذ احمد عباس كل الناس ديت الجدعنه والشهامه لا تقدر باي شيء انسان موقف والناس اللي متربيه بتفضل دائما طول عمرها متربيه تربيه جيده ضحكه في موضوع عايز اكلم حضراتكم فيه وحاول بقدر الامكان ان انا يعني اوضح نفسي جدا حضراتكم حضرته اول\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile('C:/Users/Omar Muhammed/output_audiox.wav') as source:\n",
    "    audio = r.record(source)\n",
    "\n",
    "try:\n",
    "    textA = r.recognize_google(audio, language='ar-AR')\n",
    "    print(\"Google Web Speech thinks you said (in Arabic): \" + textA)\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Web Speech could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Web Speech service; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be2411af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 16:02:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5425e66c4b6a497aaca3ee037f0c3610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 16:02:23 WARNING: Language ar package default expects mwt, which has been added\n",
      "2024-01-09 16:02:24 INFO: Loading these models for language: ar (Arabic):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | padt         |\n",
      "| mwt       | padt         |\n",
      "| ner       | aqmar_charlm |\n",
      "============================\n",
      "\n",
      "2024-01-09 16:02:24 INFO: Using device: cpu\n",
      "2024-01-09 16:02:24 INFO: Loading: tokenize\n",
      "2024-01-09 16:02:24 INFO: Loading: mwt\n",
      "2024-01-09 16:02:24 INFO: Loading: ner\n",
      "2024-01-09 16:02:24 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: عباس الصديق, Type: PER\n",
      "Entity: عبد الفتاح, Type: PER\n",
      "Entity: واحمد عباس, Type: PER\n",
      "Entity: ابوظبي, Type: LOC\n",
      "Entity: محمد, Type: PER\n",
      "Entity: احمد عباس, Type: PER\n"
     ]
    }
   ],
   "source": [
    "def analyze_text(text):\n",
    "    # Initialize the Arabic NLP pipeline with Stanza\n",
    "    nlp = stanza.Pipeline(lang='ar', processors='tokenize,ner')\n",
    "    doc = nlp(text)\n",
    "\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        print(f'Entity: {ent.text}, Type: {ent.type}')\n",
    "analyze_text(textA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78e6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER >>> PERSON \n",
    "# LOC >>> LOCATION "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
