{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87630c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc36d9",
   "metadata": {},
   "source": [
    "# Deep and wide Technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb43047",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_deep, x_train_wide = X_train[:,:5], X_train[:,2:]\n",
    "x_valid_deep, x_valid_wide = X_valid[:,:5], X_valid[:,2:]\n",
    "\n",
    "class DeepAndWide(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "        self.units = units\n",
    "        self.norm_layer_deep = tf.keras.layers.Normalization(axis=-1)\n",
    "        self.norm_layer_wide = tf.keras.layers.Normalization(axis=-1)\n",
    "        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.output_ = tf.keras.layers.Dense(1)\n",
    "        self.output_1 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        deep_inputs, wide_inputs = inputs\n",
    "        hid1 = self.hidden1(self.norm_layer_deep(deep_inputs))\n",
    "        hid2 = self.hidden2(hid1)\n",
    "        conc = self.concat([self.norm_layer_wide(wide_inputs), hid2])\n",
    "        output = self.output_(conc)\n",
    "        output1 = self.output_1(self.norm_layer_wide(wide_inputs))\n",
    "        return output, output1\n",
    "\n",
    "model = DeepAndWide(40, activation='relu')\n",
    "\n",
    "model.norm_layer_deep.adapt(x_train_deep)\n",
    "model.norm_layer_wide.adapt(x_train_wide)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9,0.1] ,optimizer=tf.keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999), metrics=['RootMeanSquaredError'])\n",
    "\n",
    "model.fit((x_train_deep, x_train_wide), (y_train, y_train), epochs=20, validation_data=((x_valid_deep, x_valid_wide), (y_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f2505",
   "metadata": {},
   "source": [
    "# using used/trained model to train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ade04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_class_id = class_names.index(\"Pullover\")\n",
    "neg_class_id = class_names.index(\"T-shirt/top\")\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
    "    y_A = y[~y_for_B]\n",
    "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
    "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
    "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
    "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
    "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_A = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"my_model_A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28b4a4",
   "metadata": {},
   "source": [
    "## Scaling the mean and standard deviation manully on the fetch-housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b602b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9c7e4",
   "metadata": {},
   "source": [
    "# hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336d612",
   "metadata": {},
   "source": [
    "# project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbe3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11763b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-27 22:56:00,545] A new study created in memory with name: no-name-ce147078-976e-4fe7-bc31-6427b3c0faf4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 1.9925 - accuracy: 0.3030INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.9923 - accuracy: 0.3031 - val_loss: 1.8162 - val_accuracy: 0.3522\n",
      "Epoch 2/100\n",
      "1397/1407 [============================>.] - ETA: 0s - loss: 1.8040 - accuracy: 0.3682INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.8037 - accuracy: 0.3682 - val_loss: 1.7412 - val_accuracy: 0.3844\n",
      "Epoch 3/100\n",
      "1393/1407 [============================>.] - ETA: 0s - loss: 1.7378 - accuracy: 0.3946INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.7378 - accuracy: 0.3950 - val_loss: 1.6966 - val_accuracy: 0.4020\n",
      "Epoch 4/100\n",
      "1391/1407 [============================>.] - ETA: 0s - loss: 1.6915 - accuracy: 0.4096INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.6919 - accuracy: 0.4094 - val_loss: 1.6594 - val_accuracy: 0.4190\n",
      "Epoch 5/100\n",
      "1394/1407 [============================>.] - ETA: 0s - loss: 1.6554 - accuracy: 0.4249INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.6548 - accuracy: 0.4252 - val_loss: 1.6396 - val_accuracy: 0.4248\n",
      "Epoch 6/100\n",
      "1397/1407 [============================>.] - ETA: 0s - loss: 1.6288 - accuracy: 0.4337INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.6291 - accuracy: 0.4338 - val_loss: 1.6230 - val_accuracy: 0.4368\n",
      "Epoch 7/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.6094 - accuracy: 0.4408INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.6095 - accuracy: 0.4407 - val_loss: 1.6074 - val_accuracy: 0.4416\n",
      "Epoch 8/100\n",
      "1398/1407 [============================>.] - ETA: 0s - loss: 1.5878 - accuracy: 0.4475INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5877 - accuracy: 0.4474 - val_loss: 1.5906 - val_accuracy: 0.4508\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.5737 - accuracy: 0.4520 - val_loss: 1.5932 - val_accuracy: 0.4512\n",
      "Epoch 10/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.5579 - accuracy: 0.4581INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.5575 - accuracy: 0.4582 - val_loss: 1.5695 - val_accuracy: 0.4578\n",
      "Epoch 11/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 1.5436 - accuracy: 0.4638INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5436 - accuracy: 0.4640 - val_loss: 1.5682 - val_accuracy: 0.4596\n",
      "Epoch 12/100\n",
      "1397/1407 [============================>.] - ETA: 0s - loss: 1.5313 - accuracy: 0.4688INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.5310 - accuracy: 0.4687 - val_loss: 1.5591 - val_accuracy: 0.4596\n",
      "Epoch 13/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.5194 - accuracy: 0.4738INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.5191 - accuracy: 0.4740 - val_loss: 1.5519 - val_accuracy: 0.4642\n",
      "Epoch 14/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 1.5114 - accuracy: 0.4813INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5115 - accuracy: 0.4812 - val_loss: 1.5452 - val_accuracy: 0.4672\n",
      "Epoch 15/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 1.5002 - accuracy: 0.4788INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5004 - accuracy: 0.4787 - val_loss: 1.5392 - val_accuracy: 0.4688\n",
      "Epoch 16/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 1.4930 - accuracy: 0.4827INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4933 - accuracy: 0.4827 - val_loss: 1.5372 - val_accuracy: 0.4680\n",
      "Epoch 17/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 1.4823 - accuracy: 0.4887INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4822 - accuracy: 0.4887 - val_loss: 1.5294 - val_accuracy: 0.4730\n",
      "Epoch 18/100\n",
      "1401/1407 [============================>.] - ETA: 0s - loss: 1.4722 - accuracy: 0.4912INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4722 - accuracy: 0.4912 - val_loss: 1.5269 - val_accuracy: 0.4742\n",
      "Epoch 19/100\n",
      "1395/1407 [============================>.] - ETA: 0s - loss: 1.4675 - accuracy: 0.4897INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4674 - accuracy: 0.4895 - val_loss: 1.5237 - val_accuracy: 0.4740\n",
      "Epoch 20/100\n",
      "1395/1407 [============================>.] - ETA: 0s - loss: 1.4583 - accuracy: 0.4970INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4586 - accuracy: 0.4968 - val_loss: 1.5158 - val_accuracy: 0.4784\n",
      "Epoch 21/100\n",
      "1400/1407 [============================>.] - ETA: 0s - loss: 1.4500 - accuracy: 0.4986INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4501 - accuracy: 0.4984 - val_loss: 1.5144 - val_accuracy: 0.4782\n",
      "Epoch 22/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.4430 - accuracy: 0.5013INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4435 - accuracy: 0.5012 - val_loss: 1.5107 - val_accuracy: 0.4798\n",
      "Epoch 23/100\n",
      "1399/1407 [============================>.] - ETA: 0s - loss: 1.4372 - accuracy: 0.5041INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4368 - accuracy: 0.5044 - val_loss: 1.5057 - val_accuracy: 0.4822\n",
      "Epoch 24/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 1.4276 - accuracy: 0.5076INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4278 - accuracy: 0.5076 - val_loss: 1.4979 - val_accuracy: 0.4832\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4212 - accuracy: 0.5073 - val_loss: 1.5017 - val_accuracy: 0.4802\n",
      "Epoch 26/100\n",
      "1394/1407 [============================>.] - ETA: 0s - loss: 1.4168 - accuracy: 0.5120INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4169 - accuracy: 0.5121 - val_loss: 1.4945 - val_accuracy: 0.4842\n",
      "Epoch 27/100\n",
      "1398/1407 [============================>.] - ETA: 0s - loss: 1.4109 - accuracy: 0.5137INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4107 - accuracy: 0.5136 - val_loss: 1.4932 - val_accuracy: 0.4854\n",
      "Epoch 28/100\n",
      "1394/1407 [============================>.] - ETA: 0s - loss: 1.4055 - accuracy: 0.5139INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.4051 - accuracy: 0.5142 - val_loss: 1.4902 - val_accuracy: 0.4834\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3986 - accuracy: 0.5165 - val_loss: 1.4912 - val_accuracy: 0.4836\n",
      "Epoch 30/100\n",
      "1397/1407 [============================>.] - ETA: 0s - loss: 1.3927 - accuracy: 0.5197INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3923 - accuracy: 0.5199 - val_loss: 1.4843 - val_accuracy: 0.4876\n",
      "Epoch 31/100\n",
      "1394/1407 [============================>.] - ETA: 0s - loss: 1.3898 - accuracy: 0.5189INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3897 - accuracy: 0.5190 - val_loss: 1.4837 - val_accuracy: 0.4842\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3811 - accuracy: 0.5240 - val_loss: 1.4848 - val_accuracy: 0.4816\n",
      "Epoch 33/100\n",
      "1396/1407 [============================>.] - ETA: 0s - loss: 1.3770 - accuracy: 0.5255INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3773 - accuracy: 0.5254 - val_loss: 1.4794 - val_accuracy: 0.4898\n",
      "Epoch 34/100\n",
      "1395/1407 [============================>.] - ETA: 0s - loss: 1.3716 - accuracy: 0.5259INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3714 - accuracy: 0.5259 - val_loss: 1.4780 - val_accuracy: 0.4912\n",
      "Epoch 35/100\n",
      "1391/1407 [============================>.] - ETA: 0s - loss: 1.3683 - accuracy: 0.5275INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3687 - accuracy: 0.5275 - val_loss: 1.4772 - val_accuracy: 0.4850\n",
      "Epoch 36/100\n",
      "1398/1407 [============================>.] - ETA: 0s - loss: 1.3629 - accuracy: 0.5298INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3624 - accuracy: 0.5300 - val_loss: 1.4716 - val_accuracy: 0.4902\n",
      "Epoch 37/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.3555 - accuracy: 0.5338INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3555 - accuracy: 0.5338 - val_loss: 1.4710 - val_accuracy: 0.4910\n",
      "Epoch 38/100\n",
      "1397/1407 [============================>.] - ETA: 0s - loss: 1.3509 - accuracy: 0.5330INFO:tensorflow:Assets written to: my_cifar10_model_0\\assets\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3511 - accuracy: 0.5328 - val_loss: 1.4683 - val_accuracy: 0.4888\n",
      "Epoch 39/100\n",
      "1174/1407 [========================>.....] - ETA: 0s - loss: 1.3476 - accuracy: 0.5348"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from pathlib import Path\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [3e-5, 1e-3, 1e-4,3e-4,3e-3,1e-2])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\", \"rmsprop\"])\n",
    "\n",
    "    # Additional hyperparameters for specific optimizers\n",
    "    if optimizer_name == \"adam\":\n",
    "        beta_1 = trial.suggest_float(\"beta_1\", 0.85, 0.95, step=0.01)\n",
    "        beta_2 = trial.suggest_float(\"beta_2\", 0.98, 0.999, step=0.001)\n",
    "        epsilon = trial.suggest_float(\"epsilon\", 1e-8, 1e-7, step=1e-8)\n",
    "        optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.0, 0.9, step=0.1)\n",
    "        nesterov = trial.suggest_categorical(\"nesterov\", [True, False])\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        rho = trial.suggest_float(\"rho\", 0.8, 0.95, step=0.05)\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.0, 0.9, step=0.1)\n",
    "        epsilon = trial.suggest_float(\"epsilon\", 1e-8, 1e-7, step=1e-8)\n",
    "        optimizer = RMSprop(learning_rate=learning_rate, rho=rho, momentum=momentum, epsilon=epsilon)\n",
    "\n",
    "    use_normalization = trial.suggest_categorical('use_normalization', [True, False])\n",
    "\n",
    "    # Build the model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "    if use_normalization:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(100, activation='selu', kernel_initializer=\"lecun_normal\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=40, restore_best_weights=True)\n",
    "    model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(f\"my_cifar10_model_{trial.number}\", save_best_only=True)\n",
    "    run_logdir = Path() / \"my_cifar10_logs\" / f\"run_{trial.number:03d}\"\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "    \n",
    "    callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "    # Train the model with callbacks\n",
    "    model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    return score[1]  # return validation accuracy\n",
    "\n",
    "# Create a study object and optimize\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the result\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: score {best_trial.value}, params {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8aec300",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(f\"my_cifar10_model_{best_trial.number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd33c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4169 - accuracy: 0.5068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.416875958442688, 0.5067999958992004]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d991158",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88105a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean, X_std = scaler.mean_, scaler.scale_  # extra code\n",
    "n_inputs = 8\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "def preprocess(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n",
    "                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n",
    "                       batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090dbfd",
   "metadata": {},
   "source": [
    "### create_example( )  this function would be used to convert data (in this case, images and labels) into an appropriate format that can be written to a TFRecord file. < steralize the tensor > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dc97ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 29s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('datasets/aclImdb')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = \"https://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "filepath = tf.keras.utils.get_file(filename, root + filename, extract=True,\n",
    "                                   cache_dir=\".\")\n",
    "path = Path(filepath).with_name(\"aclImdb\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d069525",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpath\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3c6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_dataset(directory):\n",
    "    pos_dir = os.path.join(directory, 'pos')\n",
    "    neg_dir = os.path.join(directory, 'neg')\n",
    "    \n",
    "    pos_files = [os.path.join(pos_dir, f) for f in os.listdir(pos_dir)]\n",
    "    neg_files = [os.path.join(neg_dir, f) for f in os.listdir(neg_dir)]\n",
    "    \n",
    "    all_files = pos_files + neg_files\n",
    "    all_labels = [1]*len(pos_files) + [0]*len(neg_files) # 1 for positive, 0 for negative\n",
    "    \n",
    "    return all_files, all_labels\n",
    "\n",
    "train_files, train_labels = load_dataset('datasets/aclImdb/train')\n",
    "test_files, test_labels = load_dataset('datasets/aclImdb/test')\n",
    "\n",
    "# Further split test set into validation set (15,000) and a new test set (10,000)\n",
    "valid_files = test_files[:15000]\n",
    "valid_labels = test_labels[:15000]\n",
    "\n",
    "test_files = test_files[15000:]\n",
    "test_labels = test_labels[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577ad95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(file_path, label):\n",
    "    text = tf.io.read_file(file_path)\n",
    "    return text, label\n",
    "\n",
    "def create_dataset(file_paths, labels, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(load_and_preprocess)\n",
    "    dataset = dataset.shuffle(buffer_size=25000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(train_files, train_labels)\n",
    "valid_dataset = create_dataset(valid_files, valid_labels)\n",
    "test_dataset = create_dataset(test_files, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3768a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 1000\n",
    "sample_reviews = train_dataset.map(lambda review, label: review)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens, output_mode=\"tf_idf\")\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0611580c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c2d50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 12s 11ms/step - loss: 0.4317 - accuracy: 0.8192 - val_loss: 0.4523 - val_accuracy: 0.8147\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 11s 11ms/step - loss: 0.3614 - accuracy: 0.8567 - val_loss: 0.4697 - val_accuracy: 0.8077\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 11s 10ms/step - loss: 0.3130 - accuracy: 0.8723 - val_loss: 0.3580 - val_accuracy: 0.8524\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 11s 11ms/step - loss: 0.2594 - accuracy: 0.8930 - val_loss: 0.4228 - val_accuracy: 0.8268\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 11s 10ms/step - loss: 0.2020 - accuracy: 0.9196 - val_loss: 0.3006 - val_accuracy: 0.8857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e501816f40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization,\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_dataset, epochs=5, validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd706c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def serialize_example(text, label):\n",
    "    text = text.encode('utf-8')\n",
    "    feature = {\n",
    "        'text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[text])),\n",
    "        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def tf_serialize_example(text, label):\n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        (text,label),\n",
    "        tf.string)\n",
    "    return tf.reshape(tf_string, ())\n",
    "\n",
    "# Assume train_dataset is a tf.data.Dataset of (text, label) pairs\n",
    "serialized_train_dataset = train_dataset.map(tf_serialize_example)\n",
    "filename = 'train.tfrecord'\n",
    "writer = tf.io.TFRecordWriter(filename)\n",
    "for serialized_example in serialized_train_dataset:\n",
    "    writer.write(serialized_example.numpy())\n",
    "writer.close()\n",
    "\n",
    "def parse_example(example_proto):\n",
    "    feature_description = {\n",
    "        'text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    text = example['text'].numpy().decode('utf-8')\n",
    "    label = example['label']\n",
    "    return text, label\n",
    "\n",
    "filename = 'train.tfrecord'\n",
    "raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "parsed_dataset = raw_dataset.map(parse_example)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
